{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDwrWgTRyFyaum4dPEf04H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raquib603/Basic_RoBerta_next/blob/main/SamLowe_Roberta_next.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "import numpy as np\n",
        "from scipy.special import softmax"
      ],
      "metadata": {
        "id": "nEYT6bbBG2Na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOnwmBpLGH3F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74be0bcb-d83a-4604-f04c-f67629d9a432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load model directly\n",
        "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\"SamLowe/roberta-base-go_emotions\")"
      ],
      "metadata": {
        "id": "k0rpFDfdGKcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "# Preprocess text (username and link placeholders)\n",
        "def preprocess(text):\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    return \" \".join(new_text)\n",
        "MODEL = f\"SamLowe/roberta-base-go_emotions\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "config = AutoConfig.from_pretrained(MODEL)\n",
        "# PT\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "#model.save_pretrained(MODEL)\n",
        "\n"
      ],
      "metadata": {
        "id": "UQv6I7ynHJD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Taking Input"
      ],
      "metadata": {
        "id": "_iyrczHCHuMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Covid cases are increasing fast!\"\n",
        "text = preprocess(text)\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n",
        "scores = output[0][0].detach().numpy()\n",
        "scores = softmax(scores)\n",
        "# # TF\n",
        "# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "# model.save_pretrained(MODEL)\n",
        "# text = \"Covid cases are increasing fast!\"\n",
        "# encoded_input = tokenizer(text, return_tensors='tf')\n",
        "# output = model(encoded_input)\n",
        "# scores = output[0][0].numpy()\n",
        "# scores = softmax(scores)\n",
        "# Print labels and scores\n",
        "ranking = np.argsort(scores)\n",
        "ranking = ranking[::-1]\n",
        "for i in range(scores.shape[0]):\n",
        "    l = config.id2label[ranking[i]]\n",
        "    s = scores[ranking[i]]\n",
        "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zPxi278HtgE",
        "outputId": "c18b75b9-74d0-47d8-f787-befa1281326e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) neutral 0.8032\n",
            "2) approval 0.1216\n",
            "3) excitement 0.0297\n",
            "4) admiration 0.0145\n",
            "5) realization 0.0093\n",
            "6) joy 0.0038\n",
            "7) optimism 0.0026\n",
            "8) annoyance 0.0024\n",
            "9) surprise 0.0018\n",
            "10) desire 0.0015\n",
            "11) pride 0.001\n",
            "12) love 0.0009\n",
            "13) amusement 0.0008\n",
            "14) disappointment 0.0007\n",
            "15) relief 0.0007\n",
            "16) fear 0.0007\n",
            "17) disapproval 0.0007\n",
            "18) caring 0.0006\n",
            "19) curiosity 0.0006\n",
            "20) disgust 0.0006\n",
            "21) anger 0.0005\n",
            "22) sadness 0.0004\n",
            "23) confusion 0.0004\n",
            "24) nervousness 0.0003\n",
            "25) gratitude 0.0003\n",
            "26) embarrassment 0.0002\n",
            "27) grief 0.0001\n",
            "28) remorse 0.0001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Making Function for the model input and that return a output"
      ],
      "metadata": {
        "id": "YXiAWnt_IBtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_sentiment_scores(comments_reviews):\n",
        "  text = preprocess(comments_reviews)\n",
        "  encoded_input = tokenizer(text, return_tensors='pt')\n",
        "  output = model(**encoded_input)\n",
        "  scores = output[0][0].detach().numpy()\n",
        "  scores = softmax(scores)\n",
        "  # # TF\n",
        "  # model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "  # model.save_pretrained(MODEL)\n",
        "  # text = \"Covid cases are increasing fast!\"\n",
        "  # encoded_input = tokenizer(text, return_tensors='tf')\n",
        "  # output = model(encoded_input)\n",
        "  # scores = output[0][0].numpy()\n",
        "  # scores = softmax(scores)\n",
        "  # Print labels and scores\n",
        "  ranking = np.argsort(scores)\n",
        "  ranking = ranking[::-1]\n",
        "\n",
        "  #only for 4 top values\n",
        "  #for i in range(scores.shape[0]):\n",
        "\n",
        "  for i in range(3):\n",
        "      l = config.id2label[ranking[i]]\n",
        "      s = scores[ranking[i]]\n",
        "      print(f\"{i+1}) {l} {np.round(float(s), 4)}\",  end = ' ')\n",
        "\n"
      ],
      "metadata": {
        "id": "hylfFXpKIAvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking examples with my created function\n",
        "Comment = \"you are a good boy\"\n",
        "final_sentiment_scores(Comment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6L8DreXKU9c",
        "outputId": "83a9b607-9a7b-489a-9e13-546905047bd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) admiration 0.9925 2) approval 0.0029 3) neutral 0.0013 4) joy 0.0003 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CUSTOMIZED Output"
      ],
      "metadata": {
        "id": "BTkHFbGaRRGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from scipy.special import softmax"
      ],
      "metadata": {
        "id": "ky6ctTZhRVuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_token(token):\n",
        "    if token is not None:  # Check if token is not None\n",
        "        return token.replace('Ġ', '')  # Replace Ġ with nothing (removes the space indicator)\n",
        "    return ''  # Return an empty string if token is None"
      ],
      "metadata": {
        "id": "2_8Z6-muU3KF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RED = \"\\033[91m\"\n",
        "RESET = \"\\033[0m\""
      ],
      "metadata": {
        "id": "eWD8nvUNRZCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def final_sentiment_scores(comments_reviews):\n",
        "    text = preprocess(comments_reviews)\n",
        "    encoded_input = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "    # Forward pass through the model\n",
        "    output = model(**encoded_input)\n",
        "    logits = output[0][0].detach().numpy()\n",
        "    scores = softmax(logits)\n",
        "\n",
        "    # Get the top 4 emotions\n",
        "    ranking = np.argsort(scores)[::-1]\n",
        "\n",
        "    # Print top 4 emotions and their scores\n",
        "    print(\"Top 4 emotions with scores:\")\n",
        "    for i in range(4):\n",
        "        label = config.id2label[ranking[i]]\n",
        "        score = scores[ranking[i]]\n",
        "        print(f\"{i+1}) {label}: {np.round(float(score), 4)}\")\n",
        "\n",
        "    # Now identify the tokens contributing to emotions\n",
        "    tokens = tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"][0])\n",
        "\n",
        "    # Token importance heuristic based on logits contribution\n",
        "    top_emotion_index = ranking[0]  # Focus on the top emotion first\n",
        "    token_contributions = logits[:len(tokens)]  # Simplified contribution (logits)\n",
        "\n",
        "    # Rank tokens by their contribution scores\n",
        "    ranked_tokens = np.argsort(token_contributions)[::-1]  # Sort tokens by importance\n",
        "\n",
        "    # Highlight only the top emotionally contributing tokens\n",
        "    top_token_indices = ranked_tokens[:4]  # Select top 4 contributing tokens\n",
        "\n",
        "    # Create a highlighted version of the sentence with red color\n",
        "\n",
        "\n",
        "    highlighted_sentence = ' '.join(\n",
        "        [f\"{RED}{tokens[i]}{RESET}\" if i in top_token_indices else tokens[i] for i in range(len(tokens))]\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\nHighlighted sentence with emotional text:\")\n",
        "    #call the clean function to clean my output\n",
        "    print(clean_token(highlighted_sentence))\n",
        "\n",
        "    return highlighted_sentence, scores, ranking"
      ],
      "metadata": {
        "id": "yUvV4xG1Rbcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"One person wrote five paragraphs detailing their concerns about homeless people at Stranahan Park, saying that a man had recently chased after their toy poodle.\"\n",
        "\n",
        "#store output in a different variable and then pass it through clean_token\n",
        "x = final_sentiment_scores(input_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOsVdcpaOCh1",
        "outputId": "d576c91f-8713-415a-c5f3-140d0349001a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 4 emotions with scores:\n",
            "1) neutral: 0.9869\n",
            "2) approval: 0.0023\n",
            "3) annoyance: 0.002\n",
            "4) caring: 0.0018\n",
            "\n",
            "Highlighted sentence with emotional text:\n",
            "<s> One person \u001b[91mwrote\u001b[0m \u001b[91mfive\u001b[0m \u001b[91mparagraphs\u001b[0m detailing their concerns about homeless people at St ran ahan Park , saying that a man had recently chased after their \u001b[91mtoy\u001b[0m p oodle . </s>\n"
          ]
        }
      ]
    }
  ]
}